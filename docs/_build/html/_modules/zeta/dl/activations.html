

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>zeta.dl.activations &mdash; zeta-learn 1.0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> zeta-learn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Home</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/general/introduction.html">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/general/introduction.html#dependencies">Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/general/introduction.html#features">Features</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Featured Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/tutorials/perceptron.html">The Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/tutorials/cnn.html">Convolutional Neural Networks (CNNs)</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/api/activations.html">Activation Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/activations.html#featured-activations">Featured Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/activations.html#module-zeta.dl.activations">Function Descriptions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/api/objectives.html">Objective Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/objectives.html#featured-objectives">Featured Objectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/objectives.html#module-zeta.dl.objectives">Function Descriptions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/api/optimizers.html">Optimization Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/optimizers.html#featured-optimizers">Featured Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/optimizers.html#module-zeta.dl.optimizers">Optimization Descriptions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/api/regularizers.html">Regularization Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/regularizers.html#featured-regularizers">Featured Regularizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/regularizers.html#module-zeta.ml.regularizers">Function Descriptions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/api/initializers.html">Weight Initialization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/initializers.html#featured-initializers">Featured Initializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../writeups/api/initializers.html#module-zeta.dl.initializers">Function Descriptions</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../writeups/general/support.html">Support</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">zeta-learn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>zeta.dl.activations</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for zeta.dl.activations</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<div class="viewcode-block" id="ELU"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ELU">[docs]</a><span class="k">class</span> <span class="nc">ELU</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Exponential Linear Units (ELUs)**</span>

<span class="sd">    ELUs are exponential functions which have negative values that allow them</span>
<span class="sd">    to push mean unit activations closer to zero like batch normalization but</span>
<span class="sd">    with lower computational complexity.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</span>
<span class="sd">            * [Djork-Arn√© Clevert et. al., 2016] https://arxiv.org/abs/1511.07289</span>
<span class="sd">            * [PDF] https://arxiv.org/pdf/1511.07289.pdf</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha (float32): controls the value to which an ELU saturates for negative net inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

<div class="viewcode-block" id="ELU.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ELU.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ELU activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the ELU function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">input_signal</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">))</span>

<div class="viewcode-block" id="ELU.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ELU.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ELU derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the ELU derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="SELU"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.SELU">[docs]</a><span class="k">class</span> <span class="nc">SELU</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Scaled Exponential Linear Units (SELUs)**</span>

<span class="sd">    SELUs are activations which induce self-normalizing properties and are used</span>
<span class="sd">    in Self-Normalizing Neural Networks (SNNs). SNNs enable high-level abstract</span>
<span class="sd">    representations that tend to automatically converge towards zero mean and</span>
<span class="sd">    unit variance.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Self-Normalizing Neural Networks (SELUs)</span>
<span class="sd">            * [Klambauer, G., et. al., 2017] https://arxiv.org/abs/1706.02515</span>
<span class="sd">            * [PDF] https://arxiv.org/pdf/1706.02515.pdf</span>
<span class="sd">    Args:</span>
<span class="sd">        ALPHA (float32): 1.6732632423543772848170429916717</span>
<span class="sd">        _LAMBDA (float32): 1.6732632423543772848170429916717</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">1.6732632423543772848170429916717</span>
    <span class="n">_LAMBDA</span> <span class="o">=</span> <span class="mf">1.6732632423543772848170429916717</span>

<div class="viewcode-block" id="SELU.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.SELU.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        SELU activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the SELU function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">SELU</span><span class="o">.</span><span class="n">_LAMBDA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">SELU</span><span class="o">.</span><span class="n">ALPHA</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">input_signal</span><span class="p">))</span> <span class="o">-</span> <span class="n">SELU</span><span class="o">.</span><span class="n">ALPHA</span><span class="p">)</span>

<div class="viewcode-block" id="SELU.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.SELU.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        SELU derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the SELU derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">SELU</span><span class="o">.</span><span class="n">_LAMBDA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">input_signal</span><span class="p">),</span> <span class="n">SELU</span><span class="o">.</span><span class="n">ALPHA</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Rectified Linear Units (ReLUs)**</span>

<span class="sd">    Rectifying neurons are an even better model of biological neurons yielding</span>
<span class="sd">    equal or better performance than hyperbolic tangent networks in-spite of</span>
<span class="sd">    the hard non-linearity and non-differentiability at zero hence creating</span>
<span class="sd">    sparse representations with true zeros which seem remarkably suitable</span>
<span class="sd">    for naturally sparse data.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Deep Sparse Rectifier Neural Networks</span>
<span class="sd">            * [Xavier Glorot., et. al., 2011] http://proceedings.mlr.press/v15/glorot11a.html</span>
<span class="sd">            * [PDF] http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf</span>

<span class="sd">        [2] Delving Deep into Rectifiers</span>
<span class="sd">            * [Kaiming He, et. al., 2015] https://arxiv.org/abs/1502.01852</span>
<span class="sd">            * [PDF] https://arxiv.org/pdf/1502.01852.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ReLU.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ReLU.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ReLU activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the ReLU function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

<div class="viewcode-block" id="ReLU.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ReLU.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ReLU derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the ReLU derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="TanH"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.TanH">[docs]</a><span class="k">class</span> <span class="nc">TanH</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Tangent Hyperbolic (TanH)**</span>

<span class="sd">    The Tangent Hyperbolic function is a rescaled version of the  sigmoid</span>
<span class="sd">    function that produces outputs in scale of [-1, +1]. As an activation</span>
<span class="sd">    function it produces an output for every input value hence making</span>
<span class="sd">    it a continuous function.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Hyperbolic Functions</span>
<span class="sd">            * [Mathematics Education Centre] https://goo.gl/4Dkkrd</span>
<span class="sd">            * [PDF] https://goo.gl/xPSnif</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TanH.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.TanH.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TanH activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the TanH function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)</span>

<div class="viewcode-block" id="TanH.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.TanH.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TanH derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the TanH derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">input_signal</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Sigmoid Activation Function**</span>

<span class="sd">    A Sigmoid function is often used as the output activation function for binary</span>
<span class="sd">    classification problems as it outputs values that are in the range (0, 1).</span>
<span class="sd">    Sigmoid functions are real-valued and differentiable, producing a curve</span>
<span class="sd">    that is &#39;S-shaped&#39; and feature one local minimum, and one local maximum</span>

<span class="sd">    References:</span>
<span class="sd">        [1] The influence of the sigmoid function parameters on the speed of</span>
<span class="sd">            backpropagation learning https://goo.gl/MavJjj</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Sigmoid.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Sigmoid.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sigmoid activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the Sigmoid function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">input_signal</span><span class="p">))</span>

<div class="viewcode-block" id="Sigmoid.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Sigmoid.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sigmoid derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the Sigmoid derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_signal</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)</span></div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">output_signal</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">output_signal</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="SoftPlus"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.SoftPlus">[docs]</a><span class="k">class</span> <span class="nc">SoftPlus</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **SoftPlus Activation Function**</span>

<span class="sd">    A Softplus function is a smooth approximation to the rectifier linear units</span>
<span class="sd">    (ReLUs). Near point 0, it is smooth and differentiable and produces outputs</span>
<span class="sd">    in scale of (0, +inf).</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Incorporating Second-Order Functional Knowledge for Better Option Pricing</span>
<span class="sd">            * [Charles Dugas, et. al., 2001] https://goo.gl/z3jeYc</span>
<span class="sd">            * [PDF] https://goo.gl/z3jeYc</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SoftPlus.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.SoftPlus.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        SoftPlus activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the SoftPlus function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">)</span>

<div class="viewcode-block" id="SoftPlus.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.SoftPlus.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        SoftPlus derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the SoftPlus derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">Sigmoid</span><span class="p">()</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Softmax">[docs]</a><span class="k">class</span> <span class="nc">Softmax</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Softmax Activation Function**</span>

<span class="sd">    The Softmax Activation Function is a generalization of the logistic function</span>
<span class="sd">    that squashes the outputs of each unit to real values in the range [0, 1]</span>
<span class="sd">    but it also divides each output such that the total sum of the outputs</span>
<span class="sd">    is equal to 1.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Softmax Regression</span>
<span class="sd">            [UFLDL Tutorial] https://goo.gl/1qgqdg</span>

<span class="sd">        [2] Deep Learning using Linear Support Vector Machines</span>
<span class="sd">            * [Yichuan Tang, 2015] https://arxiv.org/abs/1306.0239</span>
<span class="sd">            * [PDF] https://arxiv.org/pdf/1306.0239.pdf</span>

<span class="sd">        [3] Probabilistic Interpretation of Feedforward Network Outputs</span>
<span class="sd">            [Mario Costa, 1989] [PDF] https://goo.gl/ZhBY4r</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Softmax.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Softmax.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Softmax activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the Softmax function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">input_signal</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">))</span></div>
        <span class="k">return</span> <span class="n">probs</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<div class="viewcode-block" id="Softmax.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Softmax.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Softmax derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the Softmax derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_signal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)</span></div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">output_signal</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">output_signal</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="LeakyReLU"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.LeakyReLU">[docs]</a><span class="k">class</span> <span class="nc">LeakyReLU</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **LeakyReLU Activation Functions**</span>

<span class="sd">    Leaky ReLUs allow a small, non-zero gradient to propagate through the network</span>
<span class="sd">    when the unit is not active hence avoiding bottlenecks that can prevent</span>
<span class="sd">    learning in the Neural Network.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Rectifier Nonlinearities Improve Neural Network Acoustic Models</span>
<span class="sd">            * [Andrew L. Mass, et. al., 2013] https://goo.gl/k9fhEZ</span>
<span class="sd">            * [PDF] https://goo.gl/v48yXT</span>

<span class="sd">        [2] Empirical Evaluation of Rectified Activations in Convolutional Network</span>
<span class="sd">            * [Bing Xu, et. al., 2015] https://arxiv.org/abs/1505.00853</span>
<span class="sd">            * [PDF] https://arxiv.org/pdf/1505.00853.pdf</span>

<span class="sd">    Args:</span>
<span class="sd">        leakage (float32): provides for a small non-zero gradient (e.g. 0.01) when the unit is not active.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">leakage</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leakage</span> <span class="o">=</span> <span class="n">leakage</span>

<div class="viewcode-block" id="LeakyReLU.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.LeakyReLU.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        LeakyReLU activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the LeakyReLU function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">input_signal</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">leakage</span><span class="p">))</span>

<div class="viewcode-block" id="LeakyReLU.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.LeakyReLU.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        LeakyReLU derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the LeakyReLU derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_signal</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">leakage</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="ElliotSigmoid"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ElliotSigmoid">[docs]</a><span class="k">class</span> <span class="nc">ElliotSigmoid</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Elliot Sigmoid Activation Function**</span>

<span class="sd">    Elliot Sigmoid squashes each element of the input from the interval [-inf, inf]</span>
<span class="sd">    to the interval [-1, 1] with an &#39;S-shaped&#39; function. The fucntion is fast to</span>
<span class="sd">    calculate on simple computing hardware as it does not require any</span>
<span class="sd">    exponential or trigonometric functions</span>

<span class="sd">    References:</span>
<span class="sd">        [1] A better Activation Function for Artificial Neural Networks</span>
<span class="sd">            * [David L. Elliott, et. al., 1993] https://goo.gl/qqBdne</span>
<span class="sd">            * [PDF] https://goo.gl/fPLPcr</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ElliotSigmoid.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ElliotSigmoid.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ElliotSigmoid activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the ElliotSigmoid function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">input_signal</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_signal</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.5</span>

<div class="viewcode-block" id="ElliotSigmoid.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ElliotSigmoid.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ElliotSigmoid derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the ElliotSigmoid derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="Linear"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Linear">[docs]</a><span class="k">class</span> <span class="nc">Linear</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Linear Activation Function**</span>

<span class="sd">    Linear Activation applies identity operation on your data such that the output</span>
<span class="sd">    data is proportional to the input data. The function always returns the same</span>
<span class="sd">    value that was used as its argument.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Identity Function</span>
<span class="sd">            [Wikipedia Article] https://en.wikipedia.org/wiki/Identity_function</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Linear.activation"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Linear.activation">[docs]</a>    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Linear activation applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the Linear function applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">input_signal</span>

<div class="viewcode-block" id="Linear.derivative"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.Linear.derivative">[docs]</a>    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Linear derivative applied to input provided</span>

<span class="sd">        Args:</span>
<span class="sd">            input_signal (numpy.array): the input numpy array</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: the output of the Linear derivative applied to the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
</div>
        <span class="k">return</span> <span class="n">input_signal</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">activation_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>


<div class="viewcode-block" id="ActivationFunction"><a class="viewcode-back" href="../../../writeups/api/activations.html#zeta.dl.activations.ActivationFunction">[docs]</a><span class="k">class</span> <span class="nc">ActivationFunction</span><span class="p">:</span>

    <span class="n">_functions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;elu&#39;</span><span class="p">:</span> <span class="n">ELU</span><span class="p">,</span>
        <span class="s1">&#39;selu&#39;</span><span class="p">:</span> <span class="n">SELU</span><span class="p">,</span>
        <span class="s1">&#39;relu&#39;</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">,</span>
        <span class="s1">&#39;tanh&#39;</span><span class="p">:</span> <span class="n">TanH</span><span class="p">,</span>
        <span class="s1">&#39;linear&#39;</span><span class="p">:</span> <span class="n">Linear</span><span class="p">,</span>
        <span class="s1">&#39;identity&#39;</span><span class="p">:</span> <span class="n">Linear</span><span class="p">,</span>
        <span class="s1">&#39;sigmoid&#39;</span><span class="p">:</span> <span class="n">Sigmoid</span><span class="p">,</span>
        <span class="s1">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">Softmax</span><span class="p">,</span>
        <span class="s1">&#39;softplus&#39;</span><span class="p">:</span> <span class="n">SoftPlus</span><span class="p">,</span>
        <span class="s1">&#39;leaky-relu&#39;</span><span class="p">:</span> <span class="n">LeakyReLU</span><span class="p">,</span>
        <span class="s1">&#39;elliot-sigmoid&#39;</span><span class="p">:</span> <span class="n">ElliotSigmoid</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Activation function must be either one of the following: </span><span class="si">{}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="p">[</span><span class="n">name</span><span class="p">]()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_func</span><span class="o">.</span><span class="n">activation_name</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_func</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)</span> <span class="c1"># returns tuples</span>

    <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">):</span></div>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_func</span><span class="o">.</span><span class="n">derivative</span><span class="p">(</span><span class="n">input_signal</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, zeta-team.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'1.0.0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script> 

</body>
</html>